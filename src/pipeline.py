import json
import os
import datetime
import pandas as pd
from bs4 import BeautifulSoup
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from src.scrapers.amazon_search_spider import AmazonSearchSpider
from src.extractors.amazon_extractor import AmazonExtractor

from src.scrapers.meds_spider import MedsSearchSpider
from src.extractors.meds_extractor import MedsExtractor

from src.processors.post_processor import PostProcessor
from src.utils import get_unique_filename, copy_and_rename_json, delete_file, get_market_country_based_on_url

allowed_retailer_urls = ["amazon.de", "meds.se"]

class ScraperPipeline:
    def __init__(self, retailer_url, config):
        assert retailer_url in allowed_retailer_urls, f"not supported retailer_url: {retailer_url}, allowed: {allowed_retailer_urls}"
        
        # ä¸€ç»ç¡®è®¤ä¸èƒ½ä¿®æ”¹çš„å‚æ•°ï¼Œä¸»è¦æ˜¯çˆ¬è™«çš„åŸºæœ¬ä¿¡æ¯
        self.retailer_url = retailer_url
        self.market_country = get_market_country_based_on_url(retailer_url)
        self.search_spider, self.extractor = self.get_spider_and_extractor(retailer_url)
        
        
        # è‡ªåŠ¨æ›´æ–°çš„ä¿¡æ¯ï¼Œæ—¥æœŸå’Œè¾“å‡ºæ–‡ä»¶åï¼Œä¸ä¼šå› ä¸ºæ¯æ¬¡çš„æ£€ç´¢éœ€æ±‚å˜æ•°è€Œå˜åŒ–
        self.date = datetime.datetime.now().strftime("%m-%d")
        
        
        # éœ€è¦å¤–ç•Œè¾“å…¥çš„å‚æ•°ï¼Œç”¨äºå†³å®šæ¯æ¬¡çš„çˆ¬è™«è¡Œä¸º
        self.brand = config.get("Brand", "")
        self.category = config.get("Category", "")
        self.max_pages = config.get("Max_pages", 2)
        self.output_excel = get_unique_filename(f"./results/{self.retailer_url.replace(".", "-").lower()}_{self.date}_Brand-{self.brand}_Category-{self.category}.xlsx")        
        self.search_term = config.get("Search_term", f"{self.brand} {self.category}".strip())


    def run_scraper(self, debug=False):
        """Run Scrapy spider to collect raw HTML content from Amazon."""
        temp_scraped_data_file = "./scraped_data.json"
        output_file = get_unique_filename("./temp/scraped_data.json")
        
        if debug:
            with open(temp_scraped_data_file, "r", encoding="utf-8") as f:
                scraped_data = json.load(f)
            # åŸºç¡€åˆ†æ HTML ä¸­å•†å“å­˜åœ¨æƒ…å†µ
            cards = []
            for i, page in enumerate(scraped_data[:2]):
                html = page["html"]
                soup = BeautifulSoup(html, "html.parser")
                products = soup.select('div[data-component-type="s-search-result"]')
                if products:
                    print(f"âœ… ç¬¬ {i + 1} é¡µåŒ…å« {len(products)} ä¸ªå•†å“")
                    cards.extend(products)
                else:
                    print(f"âŒ ç¬¬ {i + 1} é¡µæœªæ‰¾åˆ°ä»»ä½•å•†å“ï¼Œå¯èƒ½è¢«åçˆ¬")
            return scraped_data
        
        print("ğŸš€ å¯åŠ¨ Scrapy çˆ¬è™«...")
        
        # åˆ é™¤æ—§çš„ä¸´æ—¶æ–‡ä»¶ä»¥é¿å…å†²çªï¼Œçˆ¬è™«ç³»ç»Ÿä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„æ–‡ä»¶ï¼Œå¦‚æœåŸå…ˆæ–‡ä»¶å°±å­˜åœ¨çš„è¯ä¼šå¯¹å†…å®¹è¿›è¡Œè¿½åŠ è€Œä¸æ˜¯è¦†ç›–å¯¼è‡´å‡ºé”™
        delete_file(temp_scraped_data_file)

        process = CrawlerProcess(get_project_settings())
        process.crawl(
            self.search_spider,
            base_url=self.retailer_url,
            search_term=self.search_term,
            max_pages=self.max_pages
        )
        process.start()

        if not os.path.exists(temp_scraped_data_file):
            print("âŒ Scraping å¤±è´¥æˆ–æœªæ‰¾åˆ°ç»“æœã€‚")
            return []

        # å¤åˆ¶å¹¶é‡å‘½å JSON æ–‡ä»¶ï¼Œç¡®ä¿å”¯ä¸€æ€§ï¼Œå¦‚æœæœªæ¥éœ€è¦å›æº¯æ•°æ®å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªæ–‡ä»¶
        copy_and_rename_json(temp_scraped_data_file, output_file)
        
        # åŠ è½½ JSON æ•°æ®ï¼Œå®Œæˆçˆ¬è™«æ£€ç´¢
        with open(output_file, "r", encoding="utf-8") as f:
            scraped_data = json.load(f)

        return scraped_data

    def extract_data(self, scraped_pages):
        all_products = []
        for page in scraped_pages:
            products = self.extractor.parse_products(page['html'])
            all_products.extend(products)
        return pd.DataFrame(all_products)

    def post_process(self, df):
        return PostProcessor().remove_duplicates(df)

    def save_to_excel(self, df):
        # æ·»åŠ å…ƒä¿¡æ¯åˆ—
        df["Date"] = self.date
        df["Market"] = self.market_country
        df["Retail"] = self.retailer_url        
        df["Brand"] = self.brand
        df["Category"] = self.category
        df["Search Keywords"] = self.search_term

        df.to_excel(self.output_excel, index=False)
        print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {self.output_excel}")

    def run_pipeline(self):
        print(f"ğŸ” æ­£åœ¨æŠ“å– '{self.search_term}' å•†å“æ•°æ®ï¼ˆå¸‚åœº: {self.retailer_url}ï¼‰...")
        scraped_pages = self.run_scraper()
        if not scraped_pages:
            print("âš ï¸ æ— æœ‰æ•ˆæŠ“å–ç»“æœï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
            return

        df = self.extract_data(scraped_pages)
        print(f"ğŸ“¦ æå–å•†å“æ€»æ•°: {len(df)}")

        df_clean = self.post_process(df)
        print(f"ğŸ§¹ å»é‡åå•†å“æ•°: {len(df_clean)}")
        
        self.save_to_excel(df_clean)

    def get_spider_and_extractor(self, retailer_url):
        if retailer_url == "amazon.de":
            return AmazonSearchSpider, AmazonExtractor
        elif retailer_url == "meds.se":
            return MedsSearchSpider, MedsExtractor()
        else:
            raise ValueError(f"Unsupported retailer URL: {retailer_url}")