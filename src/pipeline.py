import json
import os
import datetime
import pandas as pd
from bs4 import BeautifulSoup
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from src.scrapers.amazon_search_spider import AmazonSearchSpider


from src.extractors.amazon_extractor import AmazonExtractor
from src.processors.post_processor import PostProcessor
from src.utils import get_unique_filename, copy_and_rename_json, delete_file

class ScraperPipeline:
    def __init__(self, site, config):
        assert site in ['Amazon'], "Only 'Amazon' is currently supported."

        self.site = site
        self.brand = config.get("Brand", "")
        self.category = config.get("Category", "")
        self.market_country = config.get("Market", "amazon.de")
        self.max_pages = config.get("Max_pages", 2)

        # search_term è‡ªåŠ¨æ‹¼æ¥é€»è¾‘
        self.search_term = config.get("Search_term")
        if not self.search_term:
            self.search_term = f"{self.brand} {self.category}".strip()

        self.date = datetime.datetime.now().strftime("%m-%d")
        self.scraped_file = "./scraped_data.json"
        self.output_excel = get_unique_filename(f"./results/{self.site.lower()}_products_{self.date}.xlsx")

    def run_scraper(self):
        """Run Scrapy spider to collect raw HTML content from Amazon."""
        temp_scraped_data_file = "./scraped_data.json"
        output_file = get_unique_filename("./temp/scraped_data.json")
        
        print("ğŸš€ å¯åŠ¨ Scrapy çˆ¬è™«...")
        
        # åˆ é™¤æ—§çš„ä¸´æ—¶æ–‡ä»¶ä»¥é¿å…å†²çªï¼Œçˆ¬è™«ç³»ç»Ÿä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„æ–‡ä»¶ï¼Œå¦‚æœåŸå…ˆæ–‡ä»¶å°±å­˜åœ¨çš„è¯ä¼šå¯¹å†…å®¹è¿›è¡Œè¿½åŠ è€Œä¸æ˜¯è¦†ç›–å¯¼è‡´å‡ºé”™
        delete_file(temp_scraped_data_file)

        process = CrawlerProcess(get_project_settings())
        process.crawl(
            AmazonSearchSpider,
            base_url=self.market_country,
            search_term=self.search_term,
            max_pages=self.max_pages
        )
        process.start()

        if not os.path.exists(temp_scraped_data_file):
            print("âŒ Scraping å¤±è´¥æˆ–æœªæ‰¾åˆ°ç»“æœã€‚")
            return []

        # å¤åˆ¶å¹¶é‡å‘½å JSON æ–‡ä»¶ï¼Œç¡®ä¿å”¯ä¸€æ€§ï¼Œå¦‚æœæœªæ¥éœ€è¦å›æº¯æ•°æ®å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªæ–‡ä»¶
        copy_and_rename_json(temp_scraped_data_file, output_file)
        
        # åŠ è½½ JSON æ•°æ®ï¼Œå®Œæˆçˆ¬è™«æ£€ç´¢
        with open(output_file, "r", encoding="utf-8") as f:
            scraped_data = json.load(f)

        # åŸºç¡€åˆ†æ HTML ä¸­å•†å“å­˜åœ¨æƒ…å†µ
        cards = []
        for i, page in enumerate(scraped_data[:2]):
            html = page["html"]
            soup = BeautifulSoup(html, "html.parser")
            products = soup.select('div[data-component-type="s-search-result"]')
            if products:
                print(f"âœ… ç¬¬ {i + 1} é¡µåŒ…å« {len(products)} ä¸ªå•†å“")
                cards.extend(products)
            else:
                print(f"âŒ ç¬¬ {i + 1} é¡µæœªæ‰¾åˆ°ä»»ä½•å•†å“ï¼Œå¯èƒ½è¢«åçˆ¬")

        return scraped_data

    def extract_data(self, scraped_pages):
        extractor = AmazonExtractor()
        all_products = []
        for page in scraped_pages:
            products = extractor.parse_products(page['html'], self.market_country)
            all_products.extend(products)
        return pd.DataFrame(all_products)

    def post_process(self, df):
        return PostProcessor().remove_duplicates(df)

    def save_to_excel(self, df):
        # æ·»åŠ å…ƒä¿¡æ¯åˆ—
        df["Date"] = self.date
        df["Market"] = self.market_country
        df["Brand"] = self.brand
        df["Category"] = self.category
        df["Search_Term"] = self.search_term

        df.to_excel(self.output_excel, index=False)
        print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {self.output_excel}")

    def run_pipeline(self):
        print(f"ğŸ” æ­£åœ¨æŠ“å– '{self.search_term}' å•†å“æ•°æ®ï¼ˆå¸‚åœº: {self.market_country}ï¼‰...")
        scraped_pages = self.run_scraper()
        if not scraped_pages:
            print("âš ï¸ æ— æœ‰æ•ˆæŠ“å–ç»“æœï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
            return

        df = self.extract_data(scraped_pages)
        print(f"ğŸ“¦ æå–å•†å“æ€»æ•°: {len(df)}")

        df_clean = self.post_process(df)
        print(f"ğŸ§¹ å»é‡åå•†å“æ•°: {len(df_clean)}")

        self.save_to_excel(df_clean)
